
@article{zech_variable_2018,
	title = {Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: {A} cross-sectional study},
	volume = {15},
	issn = {1549-1676},
	shorttitle = {Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs},
	url = {https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1002683},
	doi = {10.1371/journal.pmed.1002683},
	abstract = {Background There is interest in using convolutional neural networks (CNNs) to analyze medical imaging to provide computer-aided diagnosis (CAD). Recent work has suggested that image classification CNNs may not generalize to new data as well as previously believed. We assessed how well CNNs generalized across three hospital systems for a simulated pneumonia screening task. Methods and findings A cross-sectional design with multiple model training cohorts was used to evaluate model generalizability to external sites using split-sample validation. A total of 158,323 chest radiographs were drawn from three institutions: National Institutes of Health Clinical Center (NIH; 112,120 from 30,805 patients), Mount Sinai Hospital (MSH; 42,396 from 12,904 patients), and Indiana University Network for Patient Care (IU; 3,807 from 3,683 patients). These patient populations had an age mean (SD) of 46.9 years (16.6), 63.2 years (16.5), and 49.6 years (17) with a female percentage of 43.5\%, 44.8\%, and 57.3\%, respectively. We assessed individual models using the area under the receiver operating characteristic curve (AUC) for radiographic findings consistent with pneumonia and compared performance on different test sets with DeLong’s test. The prevalence of pneumonia was high enough at MSH (34.2\%) relative to NIH and IU (1.2\% and 1.0\%) that merely sorting by hospital system achieved an AUC of 0.861 (95\% CI 0.855–0.866) on the joint MSH–NIH dataset. Models trained on data from either NIH or MSH had equivalent performance on IU (P values 0.580 and 0.273, respectively) and inferior performance on data from each other relative to an internal test set (i.e., new data from within the hospital system used for training data; P values both {\textless}0.001). The highest internal performance was achieved by combining training and test data from MSH and NIH (AUC 0.931, 95\% CI 0.927–0.936), but this model demonstrated significantly lower external performance at IU (AUC 0.815, 95\% CI 0.745–0.885, P = 0.001). To test the effect of pooling data from sites with disparate pneumonia prevalence, we used stratified subsampling to generate MSH–NIH cohorts that only differed in disease prevalence between training data sites. When both training data sites had the same pneumonia prevalence, the model performed consistently on external IU data (P = 0.88). When a 10-fold difference in pneumonia rate was introduced between sites, internal test performance improved compared to the balanced model (10× MSH risk P {\textless} 0.001; 10× NIH P = 0.002), but this outperformance failed to generalize to IU (MSH 10× P {\textless} 0.001; NIH 10× P = 0.027). CNNs were able to directly detect hospital system of a radiograph for 99.95\% NIH (22,050/22,062) and 99.98\% MSH (8,386/8,388) radiographs. The primary limitation of our approach and the available public data is that we cannot fully assess what other factors might be contributing to hospital system–specific biases. Conclusion Pneumonia-screening CNNs achieved better internal than external performance in 3 out of 5 natural comparisons. When models were trained on pooled data from sites with different pneumonia prevalence, they performed better on new pooled data from these sites but not on external data. CNNs robustly identified hospital system and department within a hospital, which can have large differences in disease burden and may confound predictions.},
	language = {en},
	number = {11},
	urldate = {2024-10-12},
	journal = {PLOS Medicine},
	author = {Zech, John R. and Badgeley, Marcus A. and Liu, Manway and Costa, Anthony B. and Titano, Joseph J. and Oermann, Eric Karl},
	month = jun,
	year = {2018},
	note = {Publisher: Public Library of Science},
	keywords = {Deep learning, Critical care and emergency medicine, Indiana, Inpatients, Medical risk factors, Natural language processing, Pneumonia, Radiology and imaging},
	pages = {e1002683},
	file = {Full Text PDF:/home/bjarne/Zotero/storage/3NT5GTT6/Zech et al. - 2018 - Variable generalization performance of a deep learning model to detect pneumonia in chest radiograph.pdf:application/pdf},
}

@article{mi_review_2020,
	title = {Review {Study} of {Interpretation} {Methods} for {Future} {Interpretable} {Machine} {Learning}},
	volume = {8},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9234594},
	doi = {10.1109/ACCESS.2020.3032756},
	abstract = {In recent years, black-box models have developed rapidly because of their high accuracy. Balancing the interpretability and accuracy is increasingly important. The lack of interpretability severely limits the application of the model in academia and industry. Despite the various interpretable machine learning methods, the perspective and meaning of the interpretation are also different. We provide a review of the current interpretable methods and divide them based on the model being applied. We divide them into two categories: interpretable methods with the self-explanatory model and interpretable methods with external co-explanation. And the interpretable methods with external co-explanation are further divided into subbranch methods based on instances, SHAP, knowledge graph, deep learning, and clustering model. The classification aims to help us understand the model characteristics applied in the interpretable method better. This survey makes the researcher find a suitable model to solve interpretability problems more easily. And the comparison experiments contribute to discovering complementary features from different methods. At the same time, we explore the future challenges and trends of interpretable machine learning to promote the development of interpretable machine learning.},
	urldate = {2024-10-12},
	journal = {IEEE Access},
	author = {Mi, Jian-Xun and Li, An-Di and Zhou, Li-Fang},
	year = {2020},
	note = {Conference Name: IEEE Access},
	keywords = {Deep learning, Cognition, knowledge graph, Data models, Additives, Decision trees, influential instance, interpretable machine learning, interpretable methods with external co-explanation, interpretable methods with the self-explanatory model, Predictive models},
	pages = {191969--191985},
	file = {Full Text PDF:/home/bjarne/Zotero/storage/XQV3Y9H4/Mi et al. - 2020 - Review Study of Interpretation Methods for Future Interpretable Machine Learning.pdf:application/pdf;IEEE Xplore Abstract Record:/home/bjarne/Zotero/storage/E7P72YGW/9234594.html:text/html},
}

@article{barredo_arrieta_explainable_2020,
	title = {Explainable {Artificial} {Intelligence} ({XAI}): {Concepts}, taxonomies, opportunities and challenges toward responsible {AI}},
	volume = {58},
	issn = {1566-2535},
	shorttitle = {Explainable {Artificial} {Intelligence} ({XAI})},
	url = {https://www.sciencedirect.com/science/article/pii/S1566253519308103},
	doi = {10.1016/j.inffus.2019.12.012},
	abstract = {In the last few years, Artificial Intelligence (AI) has achieved a notable momentum that, if harnessed appropriately, may deliver the best of expectations over many application sectors across the field. For this to occur shortly in Machine Learning, the entire community stands in front of the barrier of explainability, an inherent problem of the latest techniques brought by sub-symbolism (e.g. ensembles or Deep Neural Networks) that were not present in the last hype of AI (namely, expert systems and rule based models). Paradigms underlying this problem fall within the so-called eXplainable AI (XAI) field, which is widely acknowledged as a crucial feature for the practical deployment of AI models. The overview presented in this article examines the existing literature and contributions already done in the field of XAI, including a prospect toward what is yet to be reached. For this purpose we summarize previous efforts made to define explainability in Machine Learning, establishing a novel definition of explainable Machine Learning that covers such prior conceptual propositions with a major focus on the audience for which the explainability is sought. Departing from this definition, we propose and discuss about a taxonomy of recent contributions related to the explainability of different Machine Learning models, including those aimed at explaining Deep Learning methods for which a second dedicated taxonomy is built and examined in detail. This critical literature analysis serves as the motivating background for a series of challenges faced by XAI, such as the interesting crossroads of data fusion and explainability. Our prospects lead toward the concept of Responsible Artificial Intelligence, namely, a methodology for the large-scale implementation of AI methods in real organizations with fairness, model explainability and accountability at its core. Our ultimate goal is to provide newcomers to the field of XAI with a thorough taxonomy that can serve as reference material in order to stimulate future research advances, but also to encourage experts and professionals from other disciplines to embrace the benefits of AI in their activity sectors, without any prior bias for its lack of interpretability.},
	urldate = {2024-10-12},
	journal = {Information Fusion},
	author = {Barredo Arrieta, Alejandro and Díaz-Rodríguez, Natalia and Del Ser, Javier and Bennetot, Adrien and Tabik, Siham and Barbado, Alberto and Garcia, Salvador and Gil-Lopez, Sergio and Molina, Daniel and Benjamins, Richard and Chatila, Raja and Herrera, Francisco},
	month = jun,
	year = {2020},
	keywords = {Accountability, Comprehensibility, Data Fusion, Deep Learning, Explainable Artificial Intelligence, Fairness, Interpretability, Machine Learning, Privacy, Responsible Artificial Intelligence, Transparency, \#duplicate-citation-key},
	pages = {82--115},
	file = {Accepted Version:/home/bjarne/Zotero/storage/9M3878Q5/Barredo Arrieta et al. - 2020 - Explainable Artificial Intelligence (XAI) Concepts, taxonomies, opportunities and challenges toward.pdf:application/pdf;ScienceDirect Snapshot:/home/bjarne/Zotero/storage/5JSIDIMI/S1566253519308103.html:text/html},
}

@article{tinauer_interpretable_2022,
	title = {Interpretable brain disease classification and relevance-guided deep learning},
	volume = {12},
	copyright = {2022 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-022-24541-7},
	doi = {10.1038/s41598-022-24541-7},
	abstract = {Deep neural networks are increasingly used for neurological disease classification by MRI, but the networks’ decisions are not easily interpretable by humans. Heat mapping by deep Taylor decomposition revealed that (potentially misleading) image features even outside of the brain tissue are crucial for the classifier’s decision. We propose a regularization technique to train convolutional neural network (CNN) classifiers utilizing relevance-guided heat maps calculated online during training. The method was applied using T1-weighted MR images from 128 subjects with Alzheimer’s disease (mean age = 71.9 ± 8.5 years) and 290 control subjects (mean age = 71.3 ± 6.4 years). The developed relevance-guided framework achieves higher classification accuracies than conventional CNNs but more importantly, it relies on less but more relevant and physiological plausible voxels within brain tissue. Additionally, preprocessing effects from skull stripping and registration are mitigated. With the interpretability of the decision mechanisms underlying CNNs, these results challenge the notion that unprocessed T1-weighted brain MR images in standard CNNs yield higher classification accuracy in Alzheimer’s disease than solely atrophy.},
	language = {en},
	number = {1},
	urldate = {2024-10-12},
	journal = {Scientific Reports},
	author = {Tinauer, Christian and Heber, Stefan and Pirpamer, Lukas and Damulina, Anna and Schmidt, Reinhold and Stollberger, Rudolf and Ropele, Stefan and Langkammer, Christian},
	month = nov,
	year = {2022},
	note = {Publisher: Nature Publishing Group},
	keywords = {Alzheimer's disease, Biomedical engineering},
	pages = {20254},
	file = {Full Text PDF:/home/bjarne/Zotero/storage/F45GPXJB/Tinauer et al. - 2022 - Interpretable brain disease classification and relevance-guided deep learning.pdf:application/pdf},
}

@misc{kokhlikyan_captum_2020,
	title = {Captum: {A} unified and generic model interpretability library for {PyTorch}},
	shorttitle = {Captum},
	url = {http://arxiv.org/abs/2009.07896},
	doi = {10.48550/arXiv.2009.07896},
	abstract = {In this paper we introduce a novel, unified, open-source model interpretability library for PyTorch [12]. The library contains generic implementations of a number of gradient and perturbation-based attribution algorithms, also known as feature, neuron and layer importance algorithms, as well as a set of evaluation metrics for these algorithms. It can be used for both classification and non-classification models including graph-structured models built on Neural Networks (NN). In this paper we give a high-level overview of supported attribution algorithms and show how to perform memory-efficient and scalable computations. We emphasize that the three main characteristics of the library are multimodality, extensibility and ease of use. Multimodality supports different modality of inputs such as image, text, audio or video. Extensibility allows adding new algorithms and features. The library is also designed for easy understanding and use. Besides, we also introduce an interactive visualization tool called Captum Insights that is built on top of Captum library and allows sample-based model debugging and visualization using feature importance metrics.},
	urldate = {2024-10-12},
	publisher = {arXiv},
	author = {Kokhlikyan, Narine and Miglani, Vivek and Martin, Miguel and Wang, Edward and Alsallakh, Bilal and Reynolds, Jonathan and Melnikov, Alexander and Kliushkina, Natalia and Araya, Carlos and Yan, Siqi and Reblitz-Richardson, Orion},
	month = sep,
	year = {2020},
	note = {arXiv:2009.07896},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, tool},
	file = {Preprint PDF:/home/bjarne/Zotero/storage/EZJB3UTZ/Kokhlikyan et al. - 2020 - Captum A unified and generic model interpretability library for PyTorch.pdf:application/pdf;Snapshot:/home/bjarne/Zotero/storage/C4648E6A/2009.html:text/html},
}

@article{dinsdale_learning_2021,
	title = {Learning patterns of the ageing brain in {MRI} using deep convolutional networks},
	volume = {224},
	issn = {1053-8119},
	url = {https://www.sciencedirect.com/science/article/pii/S1053811920308867},
	doi = {10.1016/j.neuroimage.2020.117401},
	abstract = {Both normal ageing and neurodegenerative diseases cause morphological changes to the brain. Age-related brain changes are subtle, nonlinear, and spatially and temporally heterogenous, both within a subject and across a population. Machine learning models are particularly suited to capture these patterns and can produce a model that is sensitive to changes of interest, despite the large variety in healthy brain appearance. In this paper, the power of convolutional neural networks (CNNs) and the rich UK Biobank dataset, the largest database currently available, are harnessed to address the problem of predicting brain age. We developed a 3D CNN architecture to predict chronological age, using a training dataset of 12,802 T1-weighted MRI images and a further 6,885 images for testing. The proposed method shows competitive performance on age prediction, but, most importantly, the CNN prediction errors ΔBrainAge=AgePredicted−AgeTrue correlated significantly with many clinical measurements from the UK Biobank in the female and male groups. In addition, having used images from only one imaging modality in this experiment, we examined the relationship between ΔBrainAge and the image-derived phenotypes (IDPs) from all other imaging modalities in the UK Biobank, showing correlations consistent with known patterns of ageing. Furthermore, we show that the use of nonlinearly registered images to train CNNs can lead to the network being driven by artefacts of the registration process and missing subtle indicators of ageing, limiting the clinical relevance. Due to the longitudinal aspect of the UK Biobank study, in the future it will be possible to explore whether the ΔBrainAge from models such as this network were predictive of any health outcomes.},
	urldate = {2024-10-13},
	journal = {NeuroImage},
	author = {Dinsdale, Nicola K. and Bluemke, Emma and Smith, Stephen M. and Arya, Zobair and Vidaurre, Diego and Jenkinson, Mark and Namburete, Ana I. L.},
	month = jan,
	year = {2021},
	keywords = {Convolutional neural networks, Brain aging, UK Biobank},
	pages = {117401},
	file = {Full Text:/home/bjarne/Zotero/storage/6XA5F2SV/Dinsdale et al. - 2021 - Learning patterns of the ageing brain in MRI using deep convolutional networks.pdf:application/pdf;ScienceDirect Snapshot:/home/bjarne/Zotero/storage/MDTVJFEK/S1053811920308867.html:text/html},
}

@inproceedings{dyrba_comparison_2020,
	address = {Wiesbaden},
	title = {Comparison of {CNN} {Visualization} {Methods} to {Aid} {Model} {Interpretability} for {Detecting} {Alzheimer}’s {Disease}},
	isbn = {978-3-658-29267-6},
	doi = {10.1007/978-3-658-29267-6_68},
	abstract = {Advances in medical imaging and convolutional neural networks (CNNs) have made it possible to achieve excellent diagnostic accuracy from CNNs comparable to human raters. However, CNNs are still not implemented in medical trials as they appear as a black box system and their inner workings cannot be properly explained. Therefore, it is essential to assess CNN relevance maps, which highlight regions that primarily contribute to the prediction. This study focuses on the comparison of algorithms for generating heatmaps to visually explain the learned patterns of Alzheimer’s disease (AD) classification. T1-weighted volumetric MRI data were entered into a 3D CNN. Heatmaps were then generated for different visualization methods using the iNNvestigate and keras-vis libraries. The model reached an area under the curve of 0.93 and 0.75 for separating AD dementia patients from controls and patients with amnestic mild cognitive impairment from controls, respectively. Visualizations for the methods deep Taylor decomposition and layer-wise relevance propagation (LRP) showed most reasonable results for individual patients matching expected brain regions. Other methods, such as Grad-CAM and guided backpropagation showed more scattered activations or random areas. For clinically research, deep Taylor decomposition and LRP showed most valuable network activation patterns.},
	language = {de},
	booktitle = {Bildverarbeitung für die {Medizin} 2020},
	publisher = {Springer Fachmedien},
	author = {Dyrba, Martin and Pallath, Arjun H. and Marzban, Eman N.},
	editor = {Tolxdorff, Thomas and Deserno, Thomas M. and Handels, Heinz and Maier, Andreas and Maier-Hein, Klaus H. and Palm, Christoph},
	year = {2020},
	keywords = {relevance evaluation},
	pages = {307--312},
	file = {Full Text PDF:/home/bjarne/Zotero/storage/L6WZQEHI/Dyrba et al. - 2020 - Comparison of CNN Visualization Methods to Aid Model Interpretability for Detecting Alzheimer’s Dise.pdf:application/pdf},
}

@inproceedings{eitel_testing_2019,
	address = {Cham},
	title = {Testing the {Robustness} of {Attribution} {Methods} for {Convolutional} {Neural} {Networks} in {MRI}-{Based} {Alzheimer}’s {Disease} {Classification}},
	isbn = {978-3-030-33850-3},
	doi = {10.1007/978-3-030-33850-3_1},
	abstract = {Attribution methods are an easy to use tool for investigating and validating machine learning models. Multiple methods have been suggested in the literature and it is not yet clear which method is most suitable for a given task. In this study, we tested the robustness of four attribution methods, namely gradient * input, guided backpropagation, layer-wise relevance propagation and occlusion, for the task of Alzheimer’s disease classification. We have repeatedly trained a convolutional neural network (CNN) with identical training settings in order to separate structural MRI data of patients with Alzheimer’s disease and healthy controls. Afterwards, we produced attribution maps for each subject in the test data and quantitatively compared them across models and attribution methods. We show that visual comparison is not sufficient and that some widely used attribution methods produce highly inconsistent outcomes.},
	language = {en},
	booktitle = {Interpretability of {Machine} {Intelligence} in {Medical} {Image} {Computing} and {Multimodal} {Learning} for {Clinical} {Decision} {Support}},
	publisher = {Springer International Publishing},
	author = {Eitel, Fabian and Ritter, Kerstin},
	editor = {Suzuki, Kenji and Reyes, Mauricio and Syeda-Mahmood, Tanveer and Konukoglu, Ender and Glocker, Ben and Wiest, Roland and Gur, Yaniv and Greenspan, Hayit and Madabhushi, Anant},
	year = {2019},
	keywords = {Alzheimer’s disease, MRI, Machine learning, Convolutional neural networks, Robustness, Attribution methods, Explainability},
	pages = {3--11},
	file = {Full Text PDF:/home/bjarne/Zotero/storage/22MSP6L7/Eitel and Ritter - 2019 - Testing the Robustness of Attribution Methods for Convolutional Neural Networks in MRI-Based Alzheim.pdf:application/pdf},
}

@article{singh_evaluation_2021,
	title = {Evaluation of {Explainable} {Deep} {Learning} {Methods} for {Ophthalmic} {Diagnosis}},
	volume = {15},
	issn = {1177-5467},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8219310/},
	doi = {10.2147/OPTH.S312236},
	abstract = {Background
The lack of explanations for the decisions made by deep learning algorithms has hampered their acceptance by the clinical community despite highly accurate results on multiple problems. Attribution methods explaining deep learning models have been tested on medical imaging problems. The performance of various attribution methods has been compared for models trained on standard machine learning datasets but not on medical images. In this study, we performed a comparative analysis to determine the method with the best explanations for retinal OCT diagnosis.

Methods
A well-known deep learning model, Inception-v3 was trained to diagnose 3 retinal diseases – choroidal neovascularization (CNV), diabetic macular edema (DME), and drusen. The explanations from 13 different attribution methods were rated by a panel of 14 clinicians for clinical significance. Feedback was obtained from the clinicians regarding the current and future scope of such methods.

Results
An attribution method based on Taylor series expansion, called Deep Taylor, was rated the highest by clinicians with a median rating of 3.85/5. It was followed by Guided backpropagation (GBP), and SHapley Additive exPlanations (SHAP).

Conclusion
Explanations from the top methods were able to highlight the structures for each disease – fluid accumulation for CNV, the boundaries of edema for DME, and bumpy areas of retinal pigment epithelium (RPE) for drusen. The most suitable method for a specific medical diagnosis task may be different from the one considered best for conventional tasks. Overall, there was a high degree of acceptance from the clinicians surveyed in the study.},
	urldate = {2024-10-15},
	journal = {Clinical Ophthalmology (Auckland, N.Z.)},
	author = {Singh, Amitojdeep and Jothi Balaji, Janarthanam and Rasheed, Mohammed Abdul and Jayakumar, Varadharajan and Raman, Rajiv and Lakshminarayanan, Vasudevan},
	month = jun,
	year = {2021},
	pmid = {34177258},
	pmcid = {PMC8219310},
	pages = {2573--2581},
	file = {PubMed Central Full Text PDF:/home/bjarne/Zotero/storage/YYJ7C47W/Singh et al. - 2021 - Evaluation of Explainable Deep Learning Methods for Ophthalmic Diagnosis.pdf:application/pdf},
}

@article{arun_assessing_2021,
	title = {Assessing the {Trustworthiness} of {Saliency} {Maps} for {Localizing}                     {Abnormalities} in {Medical} {Imaging}},
	volume = {3},
	url = {https://pubs.rsna.org/doi/10.1148/ryai.2021200267},
	doi = {10.1148/ryai.2021200267},
	abstract = {Purpose

To evaluate the trustworthiness of saliency maps for abnormality localization in medical imaging.

Materials and Methods

Using two large publicly available radiology datasets (Society for Imaging Informatics in Medicine–American College of Radiology Pneumothorax Segmentation dataset and Radiological Society of North America Pneumonia Detection Challenge dataset), the performance of eight commonly used saliency map techniques were quantified in regard to (a) localization utility (segmentation and detection), (b) sensitivity to model weight randomization, (c) repeatability, and (d) reproducibility. Their performances versus baseline methods and localization network architectures were compared, using area under the precision-recall curve (AUPRC) and structural similarity index measure (SSIM) as metrics.

Results

All eight saliency map techniques failed at least one of the criteria and were inferior in performance compared with localization networks. For pneumothorax segmentation, the AUPRC ranged from 0.024 to 0.224, while a U-Net achieved a significantly superior AUPRC of 0.404 (P {\textless} .005). For pneumonia detection, the AUPRC ranged from 0.160 to 0.519, while a RetinaNet achieved a significantly superior AUPRC of 0.596 (P {\textless}.005). Five and two saliency methods (of eight) failed the model randomization test on the segmentation and detection datasets, respectively, suggesting that these methods are not sensitive to changes in model parameters. The repeatability and reproducibility of the majority of the saliency methods were worse than localization networks for both the segmentation and detection datasets.

Conclusion

The use of saliency maps in the high-risk domain of medical imaging warrants additional scrutiny and recommend that detection or segmentation models be used if localization is the desired output of the network.

Keywords: Technology Assessment, Technical Aspects, Feature Detection, Convolutional Neural Network (CNN)

Supplemental material is available for this article.

© RSNA, 2021},
	number = {6},
	urldate = {2024-10-16},
	journal = {Radiology: Artificial Intelligence},
	author = {Arun, Nishanth and Gaw, Nathan and Singh, Praveer and Chang, Ken and Aggarwal, Mehak and Chen, Bryan and Hoebel, Katharina and Gupta, Sharut and Patel, Jay and Gidwani, Mishka and Adebayo, Julius and Li, Matthew                         D. and Kalpathy-Cramer, Jayashree},
	month = nov,
	year = {2021},
	note = {Publisher: Radiological Society of North America},
	pages = {e200267},
	file = {Full Text PDF:/home/bjarne/Zotero/storage/IDVKD42A/Arun et al. - 2021 - Assessing the Trustworthiness of Saliency Maps for Localizing                     Abnormalities in M.pdf:application/pdf},
}

@article{wang_deep_2023,
	title = {Deep neural network heatmaps capture {Alzheimer}’s disease patterns reported in a large meta-analysis of neuroimaging studies},
	volume = {269},
	issn = {1053-8119},
	url = {https://www.sciencedirect.com/science/article/pii/S1053811923000770},
	doi = {10.1016/j.neuroimage.2023.119929},
	abstract = {Deep neural networks currently provide the most advanced and accurate machine learning models to distinguish between structural MRI scans of subjects with Alzheimer’s disease and healthy controls. Unfortunately, the subtle brain alterations captured by these models are difficult to interpret because of the complexity of these multi-layer and non-linear models. Several heatmap methods have been proposed to address this issue and analyze the imaging patterns extracted from the deep neural networks, but no quantitative comparison between these methods has been carried out so far. In this work, we explore these questions by deriving heatmaps from Convolutional Neural Networks (CNN) trained using T1 MRI scans of the ADNI data set and by comparing these heatmaps with brain maps corresponding to Support Vector Machine (SVM) activation patterns. Three prominent heatmap methods are studied: Layer-wise Relevance Propagation (LRP), Integrated Gradients (IG), and Guided Grad-CAM (GGC). Contrary to prior studies where the quality of heatmaps was visually or qualitatively assessed, we obtained precise quantitative measures by computing overlap with a ground-truth map from a large meta-analysis that combined 77 voxel-based morphometry (VBM) studies independently from ADNI. Our results indicate that all three heatmap methods were able to capture brain regions covering the meta-analysis map and achieved better results than SVM activation patterns. Among them, IG produced the heatmaps with the best overlap with the independent meta-analysis.},
	urldate = {2024-10-16},
	journal = {NeuroImage},
	author = {Wang, Di and Honnorat, Nicolas and Fox, Peter T. and Ritter, Kerstin and Eickhoff, Simon B. and Seshadri, Sudha and Habes, Mohamad},
	month = apr,
	year = {2023},
	keywords = {Alzheimer’s disease, Deep learning, MRI, Explainable AI, Neuroimaging, todo},
	pages = {119929},
	file = {ScienceDirect Snapshot:/home/bjarne/Zotero/storage/YPPD4D5A/S1053811923000770.html:text/html;Submitted Version:/home/bjarne/Zotero/storage/FNHKRZRQ/Wang et al. - 2023 - Deep neural network heatmaps capture Alzheimer’s disease patterns reported in a large meta-analysis.pdf:application/pdf},
}

@article{leonardsen_constructing_2024,
	title = {Constructing personalized characterizations of structural brain aberrations in patients with dementia using explainable artificial intelligence},
	volume = {7},
	copyright = {2024 The Author(s)},
	issn = {2398-6352},
	url = {https://www.nature.com/articles/s41746-024-01123-7},
	doi = {10.1038/s41746-024-01123-7},
	abstract = {Deep learning approaches for clinical predictions based on magnetic resonance imaging data have shown great promise as a translational technology for diagnosis and prognosis in neurological disorders, but its clinical impact has been limited. This is partially attributed to the opaqueness of deep learning models, causing insufficient understanding of what underlies their decisions. To overcome this, we trained convolutional neural networks on structural brain scans to differentiate dementia patients from healthy controls, and applied layerwise relevance propagation to procure individual-level explanations of the model predictions. Through extensive validations we demonstrate that deviations recognized by the model corroborate existing knowledge of structural brain aberrations in dementia. By employing the explainable dementia classifier in a longitudinal dataset of patients with mild cognitive impairment, we show that the spatially rich explanations complement the model prediction when forecasting transition to dementia and help characterize the biological manifestation of disease in the individual brain. Overall, our work exemplifies the clinical potential of explainable artificial intelligence in precision medicine.},
	language = {en},
	number = {1},
	urldate = {2024-10-16},
	journal = {npj Digital Medicine},
	author = {Leonardsen, Esten H. and Persson, Karin and Grødem, Edvard and Dinsdale, Nicola and Schellhorn, Till and Roe, James M. and Vidal-Piñeiro, Didac and Sørensen, Øystein and Kaufmann, Tobias and Westman, Eric and Marquand, Andre and Selbæk, Geir and Andreassen, Ole A. and Wolfers, Thomas and Westlye, Lars T. and Wang, Yunpeng},
	month = may,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computer science, Cognitive neuroscience, Dementia, Prognostic markers, lrp, permutation test},
	pages = {1--14},
	file = {Full Text PDF:/home/bjarne/Zotero/storage/C2GGBTPR/Leonardsen et al. - 2024 - Constructing personalized characterizations of structural brain aberrations in patients with dementi.pdf:application/pdf},
}

@article{dyrba_improving_2021,
	title = {Improving {3D} convolutional neural network comprehensibility via interactive visualization of relevance maps: evaluation in {Alzheimer}’s disease},
	volume = {13},
	issn = {1758-9193},
	shorttitle = {Improving {3D} convolutional neural network comprehensibility via interactive visualization of relevance maps},
	url = {https://doi.org/10.1186/s13195-021-00924-2},
	doi = {10.1186/s13195-021-00924-2},
	abstract = {Although convolutional neural networks (CNNs) achieve high diagnostic accuracy for detecting Alzheimer’s disease (AD) dementia based on magnetic resonance imaging (MRI) scans, they are not yet applied in clinical routine. One important reason for this is a lack of model comprehensibility. Recently developed visualization methods for deriving CNN relevance maps may help to fill this gap as they allow the visualization of key input image features that drive the decision of the model. We investigated whether models with higher accuracy also rely more on discriminative brain regions predefined by prior knowledge.},
	number = {1},
	urldate = {2024-10-21},
	journal = {Alzheimer's Research \& Therapy},
	author = {Dyrba, Martin and Hanzig, Moritz and Altenstein, Slawek and Bader, Sebastian and Ballarini, Tommaso and Brosseron, Frederic and Buerger, Katharina and Cantré, Daniel and Dechent, Peter and Dobisch, Laura and Düzel, Emrah and Ewers, Michael and Fliessbach, Klaus and Glanz, Wenzel and Haynes, John-Dylan and Heneka, Michael T. and Janowitz, Daniel and Keles, Deniz B. and Kilimann, Ingo and Laske, Christoph and Maier, Franziska and Metzger, Coraline D. and Munk, Matthias H. and Perneczky, Robert and Peters, Oliver and Preis, Lukas and Priller, Josef and Rauchmann, Boris and Roy, Nina and Scheffler, Klaus and Schneider, Anja and Schott, Björn H. and Spottke, Annika and Spruth, Eike J. and Weber, Marc-André and Ertl-Wagner, Birgit and Wagner, Michael and Wiltfang, Jens and Jessen, Frank and Teipel, Stefan J. and for the ADNI, DELCODE study groups, AIBL},
	month = nov,
	year = {2021},
	keywords = {Alzheimer’s disease, Convolutional neural network, Deep learning, Layer-wise relevance propagation, MRI},
	pages = {191},
	file = {Full Text PDF:/home/bjarne/Zotero/storage/VIXXSF2P/Dyrba et al. - 2021 - Improving 3D convolutional neural network comprehensibility via interactive visualization of relevan.pdf:application/pdf;Snapshot:/home/bjarne/Zotero/storage/GP64DIPM/s13195-021-00924-2.html:text/html},
}

@article{gaser_cat_2024,
	title = {{CAT}: a computational anatomy toolbox for the analysis of structural {MRI} data},
	volume = {13},
	issn = {2047-217X},
	shorttitle = {{CAT}},
	url = {https://doi.org/10.1093/gigascience/giae049},
	doi = {10.1093/gigascience/giae049},
	abstract = {A large range of sophisticated brain image analysis tools have been developed by the neuroscience community, greatly advancing the field of human brain mapping. Here we introduce the Computational Anatomy Toolbox (CAT)—a powerful suite of tools for brain morphometric analyses with an intuitive graphical user interface but also usable as a shell script. CAT is suitable for beginners, casual users, experts, and developers alike, providing a comprehensive set of analysis options, workflows, and integrated pipelines. The available analysis streams—illustrated on an example dataset—allow for voxel-based, surface-based, and region-based morphometric analyses. Notably, CAT incorporates multiple quality control options and covers the entire analysis workflow, including the preprocessing of cross-sectional and longitudinal data, statistical analysis, and the visualization of results. The overarching aim of this article is to provide a complete description and evaluation of CAT while offering a citable standard for the neuroscience community.},
	urldate = {2024-10-22},
	journal = {GigaScience},
	author = {Gaser, Christian and Dahnke, Robert and Thompson, Paul M and Kurth, Florian and Luders, Eileen and {the Alzheimer's Disease Neuroimaging Initiative}},
	month = jan,
	year = {2024},
	pages = {giae049},
	file = {Full Text PDF:/home/bjarne/Zotero/storage/HKDC68M8/Gaser et al. - 2024 - CAT a computational anatomy toolbox for the analysis of structural MRI data.pdf:application/pdf;Snapshot:/home/bjarne/Zotero/storage/E2GPKABA/7727520.html:text/html},
}

@misc{petsiuk_rise_2018,
	title = {{RISE}: {Randomized} {Input} {Sampling} for {Explanation} of {Black}-box {Models}},
	shorttitle = {{RISE}},
	url = {http://arxiv.org/abs/1806.07421},
	doi = {10.48550/arXiv.1806.07421},
	abstract = {Deep neural networks are being used increasingly to automate data analysis and decision making, yet their decision-making process is largely unclear and is difficult to explain to the end users. In this paper, we address the problem of Explainable AI for deep neural networks that take images as input and output a class probability. We propose an approach called RISE that generates an importance map indicating how salient each pixel is for the model's prediction. In contrast to white-box approaches that estimate pixel importance using gradients or other internal network state, RISE works on black-box models. It estimates importance empirically by probing the model with randomly masked versions of the input image and obtaining the corresponding outputs. We compare our approach to state-of-the-art importance extraction methods using both an automatic deletion/insertion metric and a pointing metric based on human-annotated object segments. Extensive experiments on several benchmark datasets show that our approach matches or exceeds the performance of other methods, including white-box approaches. Project page: http://cs-people.bu.edu/vpetsiuk/rise/},
	urldate = {2024-10-22},
	publisher = {arXiv},
	author = {Petsiuk, Vitali and Das, Abir and Saenko, Kate},
	month = sep,
	year = {2018},
	note = {arXiv:1806.07421},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, evaluation},
	file = {Preprint PDF:/home/bjarne/Zotero/storage/9WKBX53W/Petsiuk et al. - 2018 - RISE Randomized Input Sampling for Explanation of Black-box Models.pdf:application/pdf;Snapshot:/home/bjarne/Zotero/storage/6PMJ9U6G/1806.html:text/html},
}

@article{bach_pixel-wise_2015,
	title = {On {Pixel}-{Wise} {Explanations} for {Non}-{Linear} {Classifier} {Decisions} by {Layer}-{Wise} {Relevance} {Propagation}},
	volume = {10},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0130140},
	doi = {10.1371/journal.pone.0130140},
	abstract = {Understanding and interpreting classification decisions of automated image classification systems is of high value in many applications, as it allows to verify the reasoning of the system and provides additional information to the human expert. Although machine learning methods are solving very successfully a plethora of tasks, they have in most cases the disadvantage of acting as a black box, not providing any information about what made them arrive at a particular decision. This work proposes a general solution to the problem of understanding classification decisions by pixel-wise decomposition of nonlinear classifiers. We introduce a methodology that allows to visualize the contributions of single pixels to predictions for kernel-based classifiers over Bag of Words features and for multilayered neural networks. These pixel contributions can be visualized as heatmaps and are provided to a human expert who can intuitively not only verify the validity of the classification decision, but also focus further analysis on regions of potential interest. We evaluate our method for classifiers trained on PASCAL VOC 2009 images, synthetic image data containing geometric shapes, the MNIST handwritten digits data set and for the pre-trained ImageNet model available as part of the Caffe open source package.},
	language = {en},
	number = {7},
	urldate = {2024-10-22},
	journal = {PLOS ONE},
	author = {Bach, Sebastian and Binder, Alexander and Montavon, Grégoire and Klauschen, Frederick and Müller, Klaus-Robert and Samek, Wojciech},
	month = oct,
	year = {2015},
	note = {Publisher: Public Library of Science},
	keywords = {Neural networks, Algorithms, Coding mechanisms, Imaging techniques, Kernel functions, Neurons, Sensory perception, Vision},
	pages = {e0130140},
	file = {Full Text PDF:/home/bjarne/Zotero/storage/BP68AJMD/Bach et al. - 2015 - On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation.pdf:application/pdf},
}

@misc{shrikumar_learning_2019,
	title = {Learning {Important} {Features} {Through} {Propagating} {Activation} {Differences}},
	url = {http://arxiv.org/abs/1704.02685},
	abstract = {The purported “black box” nature of neural networks is a barrier to adoption in applications where interpretability is essential. Here we present DeepLIFT (Deep Learning Important FeaTures), a method for decomposing the output prediction of a neural network on a speciﬁc input by backpropagating the contributions of all neurons in the network to every feature of the input. DeepLIFT compares the activation of each neuron to its ‘reference activation’ and assigns contribution scores according to the difference. By optionally giving separate consideration to positive and negative contributions, DeepLIFT can also reveal dependencies which are missed by other approaches. Scores can be computed efﬁciently in a single backward pass. We apply DeepLIFT to models trained on MNIST and simulated genomic data, and show signiﬁcant advantages over gradient-based methods. Video tutorial: http://goo.gl/qKb7pL, ICML slides: bit.ly/deeplifticmlslides, ICML talk: https://vimeo.com/238275076, code: http://goo.gl/RM8jvH.},
	language = {en},
	urldate = {2024-10-22},
	publisher = {arXiv},
	author = {Shrikumar, Avanti and Greenside, Peyton and Kundaje, Anshul},
	month = oct,
	year = {2019},
	note = {arXiv:1704.02685 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
	file = {PDF:/home/bjarne/Zotero/storage/DMSB3B4I/Shrikumar et al. - 2019 - Learning Important Features Through Propagating Activation Differences.pdf:application/pdf},
}

@inproceedings{ansel_pytorch_2024,
	title = {{PyTorch} 2: {Faster} {Machine} {Learning} {Through} {Dynamic} {Python} {Bytecode} {Transformation} and {Graph} {Compilation}},
	url = {https://pytorch.org/assets/pytorch2-2.pdf},
	doi = {10.1145/3620665.3640366},
	booktitle = {29th {ACM} {International} {Conference} on {Architectural} {Support} for {Programming} {Languages} and {Operating} {Systems}, {Volume} 2 ({ASPLOS} '24)},
	publisher = {ACM},
	author = {Ansel, Jason and Yang, Edward and He, Horace and Gimelshein, Natalia and Jain, Animesh and Voznesensky, Michael and Bao, Bin and Bell, Peter and Berard, David and Burovski, Evgeni and Chauhan, Geeta and Chourdia, Anjali and Constable, Will and Desmaison, Alban and DeVito, Zachary and Ellison, Elias and Feng, Will and Gong, Jiong and Gschwind, Michael and Hirsh, Brian and Huang, Sherlock and Kalambarkar, Kshiteej and Kirsch, Laurent and Lazos, Michael and Lezcano, Mario and Liang, Yanbo and Liang, Jason and Lu, Yinghai and Luk, CK and Maher, Bert and Pan, Yunjie and Puhrsch, Christian and Reso, Matthias and Saroufim, Mark and Siraichi, Marcos Yukio and Suk, Helen and Suo, Michael and Tillet, Phil and Wang, Eikan and Wang, Xiaodong and Wen, William and Zhang, Shunting and Zhao, Xu and Zhou, Keren and Zou, Richard and Mathews, Ajit and Chanan, Gregory and Wu, Peng and Chintala, Soumith},
	month = apr,
	year = {2024},
}

@misc{ancona_towards_2018,
	title = {Towards better understanding of gradient-based attribution methods for {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1711.06104},
	abstract = {Understanding the ﬂow of information in Deep Neural Networks (DNNs) is a challenging problem that has gain increasing attention over the last few years. While several methods have been proposed to explain network predictions, there have been only a few attempts to compare them from a theoretical perspective. What is more, no exhaustive empirical comparison has been performed in the past. In this work, we analyze four gradient-based attribution methods and formally prove conditions of equivalence and approximation between them. By reformulating two of these methods, we construct a uniﬁed framework which enables a direct comparison, as well as an easier implementation. Finally, we propose a novel evaluation metric, called Sensitivity-n and test the gradient-based attribution methods alongside with a simple perturbation-based attribution method on several datasets in the domains of image and text classiﬁcation, using various network architectures.},
	language = {en},
	urldate = {2024-11-03},
	publisher = {arXiv},
	author = {Ancona, Marco and Ceolini, Enea and Öztireli, Cengiz and Gross, Markus},
	month = mar,
	year = {2018},
	note = {arXiv:1711.06104 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {PDF:/home/bjarne/Zotero/storage/94VTUVXF/Ancona et al. - 2018 - Towards better understanding of gradient-based attribution methods for Deep Neural Networks.pdf:application/pdf},
}

@misc{noauthor_algorithm_nodate,
	title = {Algorithm {Descriptions} · {Captum}},
	url = {https://captum.ai/},
	abstract = {Captum is a library within which different interpretability methods can be implemented.  The Captum team welcomes any contributions in the form of algorithms, methods or library extensions!},
	language = {en},
	urldate = {2024-11-04},
	file = {Snapshot:/home/bjarne/Zotero/storage/LQ7MELCK/attribution_algorithms.html:text/html},
}

@article{brima_saliency-driven_2024,
	title = {Saliency-driven explainable deep learning in medical imaging: bridging visual explainability and statistical quantitative analysis},
	volume = {17},
	issn = {1756-0381},
	shorttitle = {Saliency-driven explainable deep learning in medical imaging},
	url = {https://doi.org/10.1186/s13040-024-00370-4},
	doi = {10.1186/s13040-024-00370-4},
	abstract = {Deep learning shows great promise for medical image analysis but often lacks explainability, hindering its adoption in healthcare. Attribution techniques that explain model reasoning can potentially increase trust in deep learning among clinical stakeholders. In the literature, much of the research on attribution in medical imaging focuses on visual inspection rather than statistical quantitative analysis.},
	number = {1},
	urldate = {2025-01-13},
	journal = {BioData Mining},
	author = {Brima, Yusuf and Atemkeng, Marcellin},
	month = jun,
	year = {2024},
	pages = {18},
	file = {Full Text PDF:/home/bjarne/Zotero/storage/SR9M4G5L/Brima and Atemkeng - 2024 - Saliency-driven explainable deep learning in medical imaging bridging visual explainability and sta.pdf:application/pdf;Snapshot:/home/bjarne/Zotero/storage/DCM53F96/s13040-024-00370-4.html:text/html},
}
